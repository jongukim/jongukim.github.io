#+TITLE: NVIDIA Apex
#+OPTIONS: ^:nil

Mixed precision을 편리하게 사용할 수 있는 apex.amp와 병렬처리를 편하게 할 수 있는 apex.parallel로 구성.
현재 버전은 0.1이며, [[https://nvidia.github.io/apex/amp.html][문서]]도 제공 중.

* apex.amp

몇 줄만 추가하면 FP16을 쉽게 활용할수 있다.

** model과 optimizer

#+BEGIN_SRC python
from apex import amp

model = ...
optimizer = ...
model, optimizer = amp.initialize(model, optimizer, opt_level="O2")
#+END_SRC

위와 같이 모델과 optimizer를 선언한 후 amp.initialize로 한 번 감싸주면 끝.
optlevel은 O0 ~ O3가 있다.

- O0: FP32
- O1: Mixed precision
- O2: Almost FP16
- O3: FP16

** loss.backward()

변경할 것이 하나 더 있다.
loss.backward() 코드를 다음과 같이 바꿔주면 된다.

#+BEGIN_SRC python
with amp.scale_loss(loss, optimizer) as scaled_loss:
  scaled_loss.backward()
#+END_SRC

* apex.parallel

현재 내 환경은 싱글 노드 multi-GPU다.
싱글 노드에서도 DataParallel보다 DistributedDataParallel(DDP)을 사용하는 것이 multi-GPU를 효율적으로 사용할 수 있다고 문서에 기술되어 있다.
DDP를 사용하려면 코드를 제법 고쳐야 하는데, apex는 조금 더 wrapping된 DPP를 제공한다.

** 모델 부분

DataParallel을 사용할 때와 마찬가지로 다음과 같이 모델을 감싸주면 모델에 대한 병렬처리가 가능하도록 해준다.
#+BEGIN_SRC python
from apex.parallel import DistributedDataParallel as DDP

model = DDP(model, delay_allreduce=True)
#+END_SRC

delay_allreduce는 기본적으로 False인데, 많은 예제에서 True로 설정하고 있어서 그냥 따라했다.

학습을 실행하는 파이썬 스크립트에는 변경이 조금 필요하다.
DDP는 torch.distributed.launch 모듈을 통해 multi-process로 실행한다.
실행 명렁어는 다음과 같은 형태이다.

** 실행 스크립트

#+BEGIN_SRC shell
python3 -m torch.distributed.launch --nproc_per_node=6 train.py ...
#+END_SRC

--nproc_per_node는 로컬 머신에 있는 GPU 개수를 전달하면 된다.
나는 6개의 V100 GPU를 가지고 있다.

위와 같이 명령어를 실행하면 자동으로 여러 개의 train.py 프로세스가 생생되는데 --local_rank라는 인자를 자동으로 붙여서 실행하게 된다.
따라서 train.py 스크립트는 다음과 같이 --local_rank 인자를 받아야 한다.

#+BEGIN_SRC python
import argparse

parser = argparse.ArgumentParser(description="training script")
parser.add_argument("--local_rank", type=int, default=0)
...
args = parser.parse_args()
rank = args.local_rank
#+END_SRC

그리고 각 스크립트가 넘겨받은 인자에 따라 GPU를 사용하도록 아래와 같이 설정한다.
#+BEGIN_SRC python
torch.cuda.set_device(rank)
#+END_SRC

GPU간 통신은 NCCL을 사용하도록 한다.
#+BEGIN_SRC python
import torch.distributed as dist

dist.init_process_group(backend="nccl")
#+END_SRC

** DataLoader와 DistributedSampler

또 하나 변경해주어야 하는 것은 DataLoader 부분이다.
분산 스크립트가 데이터를 나누어 로드할 수 있도록 Sampler로 감싸준다.

#+BEGIN_SRC python
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler

dataset = ...
samper = DistributedSampler(dataset)
data_loader = DataLoader(dataset, sampler=sampler, ...)
#+END_SRC

dataset은 본래 CPU 학습이나 단일 GPU 사용 시와 마찬가지로  torch.utils.data.Dataset을 상속 받은 인스턴스를 사용하면 된다.
DataLoader에는 num_workers를 지정해주는 것도 속도 개선이 도움이 되었다.

** Logging

여러 스크립트가 실행되기 때문에 로그 출력 코드를 그대로 사용하면 출력이 중복된다.

#+BEGIN_SRC python
if rank == 0:
  logger.info(...)
#+END_SRC

위와 같은 형태로 rank가 0일 때만 출력하면 된다.

** Model saving/loading

마찬가지로 모델 저장도 rank가 0일 때만 처리하면 된다.
그리고 현재 모델이 DDP로 한 번 감싸져 있으므로 만약 multi-GPU가 아닌 다른 환경에서 weights를 로드하고 싶다면,

#+BEGIN_SRC python
torch.save({"model": model.state_dict(),
            "params": model.module.state_dict(),
	    "optim": optimizer.state_dict()})
#+END_SRC

와 같이 저장해서 model과 optim은 학습을 이어갈 때나 multi-GPU 환경에서 inference할 때 사용하고, CPU나 단일 GPU 환경에서 사용할 때는 params를 로드하여 사용하면 된다.
